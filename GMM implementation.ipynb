{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model\n",
    "<b>Zhiling Tan</b>\n",
    "\n",
    "\n",
    "This notebook includes two parts:\n",
    "1. The solution to the assignment based on my understanding.\n",
    "2. An implementation of Gaussian Mixture Models in Pytorch. \n",
    "\n",
    "\n",
    "## 1. pytorch auto-grad GMM\n",
    "#### Assignment:\n",
    "Can you create a Gaussian Mixture Model (GMM) and use it to generate samples as tensors that are linked to the parameters of the GMM? That is the gradient of an objective function, which is calculated using the samples, can be propagated to the parameters of the GMM.\n",
    "\n",
    "#### To solve this problem:\n",
    "\n",
    "(1). generated means and covariance matrix(contains correlation coefficients) randomly.\n",
    "\n",
    "(2). create a GMM using <b>MultivariateNormal</b> and <b>MixtureSameFamily</b> class from <b>torch.distributions</b> package, based on the means and covariance generated in step1.\n",
    "\n",
    "(3). set samples' attribute <b>.requires_grad</b> as <b>True</b> and define an objective function. \n",
    "\n",
    "(4) call <b>backward( )</b> & calculate the gradient.\n",
    "\n",
    "<b>PS</b>: I don't understand what the objective function of GMM should be. (EM algorithm don't require obejective function) So I assume the objective function as <i>mean( )</i> for the test.\n",
    "\n",
    "\n",
    "#### reference:\n",
    "\n",
    "https://pytorch.org/docs/stable/distributions.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0025, 0.0025],\n",
      "        [0.0025, 0.0025],\n",
      "        [0.0025, 0.0025],\n",
      "        [0.0025, 0.0025],\n",
      "        [0.0025, 0.0025],\n",
      "        [0.0025, 0.0025],\n",
      "        [0.0025, 0.0025],\n",
      "        [0.0025, 0.0025],\n",
      "        [0.0025, 0.0025]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributions as D\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "GMM with random mu & covariance & random pi allowing gradient\n",
    "args:\n",
    "    n_components:   number of mixed components\n",
    "    n_attributes:   number of mixed attributes\n",
    "paramteters:\n",
    "    mix:            distribution of mixed components  \n",
    "                    class: Categorical - a categorical distribution parameterized by probs\n",
    "                    shape: tensor(n_components)\n",
    "    mu:             mean of attributes of each components\n",
    "                    shape: tensor(n_components, n_attributes)\n",
    "    sigma:          covariance matrix of each components\n",
    "                    shape: tensor(n_components, n_attributes, n_attributes)\n",
    "\"\"\"\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "n_components = 3\n",
    "n_attributes = 2\n",
    "\n",
    "\n",
    "# components distribution\n",
    "mix = D.categorical.Categorical(torch.abs(torch.rand(n_components))) \n",
    "\n",
    "# - random means\n",
    "mu = torch.randn(n_components, n_attributes)\n",
    "\n",
    "# - random covariance matrix\n",
    "def randn_sigma(n_components, n_attributes):\n",
    "    x = torch.rand(n_components, n_attributes, n_attributes)\n",
    "    x_t = torch.transpose(x, 1, 2)\n",
    "    sigma = torch.empty(n_components, n_attributes, n_attributes)\n",
    "    for i in range(0, n_components):\n",
    "        sigma[i] = torch.mm(x[i], x_t[i])\n",
    "    return(sigma)\n",
    "sigma = randn_sigma(n_components, n_attributes)\n",
    "\n",
    "\n",
    "# component distributions\n",
    "comp = D.multivariate_normal.MultivariateNormal(mu, sigma) \n",
    "# mixture distributions\n",
    "gmm = D.mixture_same_family.MixtureSameFamily(mix, comp)\n",
    "\n",
    "\n",
    "# generate 200 sample from GMM model & require dradient\n",
    "y= gmm.sample([200])\n",
    "y.requires_grad_(True)\n",
    "\n",
    "\n",
    "# test gradient: \n",
    "# assume obj = mean(y); that is obj = [1/(n_attributes * dim)] * y \n",
    "# derivate = 1/n_attributes \n",
    "obj =  y.mean()\n",
    "obj.backward()\n",
    "print(y.grad[1:10,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  GMM implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import math\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GMM model:\n",
    "For simplicity, I assume that all attributes（dimensions）are independent of each other（no correlation coefficients).\n",
    "\n",
    "#### reference:\n",
    "https://github.com/ldeecke/gmm-torch/blob/master/gmm.py\n",
    "\n",
    "https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_dimentions, mu_init=None, var_init=None, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Gaussian Mixuture Model:\n",
    "        mixture model of n_features multivariate normal distributions\n",
    "        args:\n",
    "            x:              [optional] input sample data \n",
    "                            shape: tensor(n_samles, n_dimentions) \n",
    "            n_features:   number of mixiture distributions (features/clusters).\n",
    "            n_dimentions:   number of attributes (dimentions).\n",
    "            \n",
    "            epsolion:       float, for avoiding divide by zero\n",
    "        parameters:\n",
    "            mu:             The mean of each mixture component.\n",
    "                            shape: tensor(1, n_features, n_dimentions)\n",
    "            var:          the covariance of each mixture component.\n",
    "                            shape: tensor(1, n_features, n_dimentions, n_dimentions)\n",
    "        \"\"\"\n",
    "        \n",
    "        # model parameters\n",
    "        super(GMM, self).__init__()\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.n_dimentions = n_dimentions\n",
    "        self.mu = mu\n",
    "        self.var = var\n",
    "        self.logvar = self.var.log()\n",
    "        self.eps=epsilon\n",
    "        \n",
    "        self.pi = torch.empty(1,n_features,1)\n",
    "        self.pi.data = (torch.ones(1,n_features,1))/n_features #equally likely mixture features\n",
    "        \n",
    "    \n",
    "    def _log_gaussian(self, x, mu=0, logvar=0.):\n",
    "        \"\"\"\n",
    "        Returns the component-wise density of x under the gaussian parameterised by `mean` and `logvar`\n",
    "        \"\"\"\n",
    "        log_norm_constant = -0.5 * np.log(2 * np.pi)\n",
    "        if type(logvar) == 'float':\n",
    "            logvar = x.new(1).fill_(logvar)\n",
    "        a = (x - mu) ** 2\n",
    "        log_p = -0.5 * (logvar + a / logvar.exp())\n",
    "        log_p = log_p + log_norm_constant\n",
    "\n",
    "        return log_p\n",
    "    \n",
    "    def _log_likelihoods(self, x, logvar):\n",
    "        # (K, samples, features)\n",
    "        mu = self.mu\n",
    "        log_likelihood = self._log_gaussian(\n",
    "            x[None, :, :], #(1, samples, features)\n",
    "            mu[:, None, :], #(K, 1, features)\n",
    "            logvar[:, None, :]#(K, 1, features)\n",
    "        )\n",
    "        log_likelihood = log_likelihood.sum(-1)\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def _cal_posteriors(self, log_likelihood):\n",
    "        posteriors = log_likelihood \n",
    "        posteriors = posteriors - torch.logsumexp(posteriors, dim=0, keepdim=True)\n",
    "        return posteriors\n",
    "    \n",
    "    def _updata_param(self, x, log_posteriors, eps=1e-6):\n",
    "        posteriors = log_posteriors.exp()\n",
    "\n",
    "        K = posteriors.size(0)\n",
    "        N_k = torch.sum(posteriors, dim=1) # (K)\n",
    "        N_k = N_k.view(K, 1, 1)\n",
    "        \n",
    "        print(\"num of point that is assigned to each gaussian: \")\n",
    "        print(N_k)\n",
    "\n",
    "        \n",
    "        # (K, 1, examples) @ (1, examples, features) -> (K, 1, features)\n",
    "        mu = posteriors[:, None] @ x[None,]\n",
    "        mu = mu / (N_k + eps)\n",
    "\n",
    "      \n",
    "        y = x.unsqueeze(0) - mu\n",
    "        var = posteriors[:, None] @ (y * y) # (K, 1, features)\n",
    "        var = var / (N_k + eps)\n",
    "        logvar = torch.clamp(var, min=1e-6).log()\n",
    "\n",
    "        # recompute the mixing probabilities\n",
    "        pi = N_k / N_k.sum()\n",
    "\n",
    "        return mu.squeeze(1), logvar.squeeze(1), pi.squeeze()\n",
    "        \n",
    "    \n",
    "    def fitGMM(self, x, n_iter=50, err=1e-8):\n",
    "\n",
    "        pre_cost = float('inf')\n",
    "        for i in range(n_iter):\n",
    "            log_likelihoods = self._log_likelihoods(x, self.logvar)\n",
    "            log_posteriors = self._cal_posteriors(log_likelihoods)\n",
    "            \n",
    "            cost = log_likelihoods.mean()\n",
    "            diff = pre_cost - cost\n",
    "            if torch.abs(diff).item() < err:\n",
    "                print(\"fit done!\")\n",
    "                break\n",
    "            pre_cost = cost\n",
    "            \n",
    "            self.mu, self.logvar, self.pi = self._updata_param(x, log_posteriors)\n",
    "            print(i,\" iter:\")\n",
    "            print(\"mu: \", self.mu)\n",
    "            print(\"pi: \", self.pi)\n",
    "            print(\"logvar: \", self.logvar)\n",
    "            print(\"----------------------------------\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialze paramaters \n",
    "Improvement method: using the results of K-Means as an initial value for $\\mu_k$\n",
    "\n",
    "(maybe implement it later? and compara which one can achieve better results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(data, features, var=1):\n",
    "    \"\"\"\n",
    "    Randomly initialize the parameters for `k` gaussians.\n",
    "    \"\"\"\n",
    "    # choose k points from data to initialize means\n",
    "    m = data.size(0)\n",
    "    i = torch.from_numpy(np.random.choice(m, features, replace=False))\n",
    "    mu = data[i]\n",
    "\n",
    "    # uniform sampling for means and variances\n",
    "    var = torch.Tensor(features, dim).fill_(var)\n",
    "    \n",
    "    # equally likely mixture components\n",
    "    pi = torch.empty(features).fill_(1. / features)\n",
    "\n",
    "    return mu, var, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate samples from random GMM\n",
    "#### Reference:\n",
    "https://github.com/aakhundov/tf-example-models/blob/master/models/tf_kmeans.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS = 2\n",
    "CLUSTERS = 3\n",
    "DATA_POINTS = 1000\n",
    "def generate_gmm_data(points, components, dimensions):\n",
    "    \"\"\"Generates synthetic data of a given size from a random GMM\"\"\"\n",
    "    np.random.seed(219)\n",
    "\n",
    "    c_means = np.random.normal(size=[components, dimensions]) * 10\n",
    "    c_variances = np.abs(np.random.normal(size=[components, dimensions]))\n",
    "    c_weights = np.abs(np.random.normal(size=[components]))\n",
    "    c_weights /= np.sum(c_weights)\n",
    "\n",
    "    result = np.zeros((points, 1, dimensions), dtype=np.float32)\n",
    "\n",
    "    for i in range(points):\n",
    "        comp = np.random.choice(np.array(range(CLUSTERS)), p=c_weights)\n",
    "        result[i] = np.random.multivariate_normal(\n",
    "            c_means[comp], np.diag(c_variances[comp])\n",
    "        )\n",
    "\n",
    "    np.random.seed()\n",
    "\n",
    "    return result, c_means, c_variances, c_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5863,  4.9325],\n",
      "        [10.7978, -2.8794],\n",
      "        [ 2.0587,  6.3497],\n",
      "        ...,\n",
      "        [10.0919, -1.8412],\n",
      "        [ 3.1869,  5.7168],\n",
      "        [ 2.6392,  3.6349]], requires_grad=True)\n",
      "---------------------------------\n",
      "mean:  [[-5.71369614 -7.99577437]\n",
      " [ 2.59275506  5.30706069]\n",
      " [10.79572809 -2.44346959]]\n",
      "---------------------------------\n",
      "pi:  [0.11936217 0.65504692 0.22559091]\n",
      "---------------------------------\n",
      "log_var: [[ 0.41284553 -1.40222891]\n",
      " [-0.67235534 -0.37121755]\n",
      " [-1.72339317 -0.24756056]]\n"
     ]
    }
   ],
   "source": [
    "sample, true_means, true_variances, true_weights = generate_gmm_data(DATA_POINTS, CLUSTERS, DIMENSIONS)\n",
    "data = torch.tensor(sample)\n",
    "data = data.squeeze(1)\n",
    "data.requires_grad_(True)  # auto-grad\n",
    "print(data)\n",
    "print(\"---------------------------------\")\n",
    "print(\"mean: \", true_means)\n",
    "print(\"---------------------------------\")\n",
    "print(\"pi: \", true_weights)\n",
    "print(\"---------------------------------\")\n",
    "print(\"log_var:\", np.log(true_variances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdmElEQVR4nO3df5Ac9Znf8fezowHPEo4VZsOhMQKVj1rHOgUtbHFcVHEZDiN+GGst7ANCLs7lqnROmapgO1tZLhd+XHwl5RT/yAWXCc65cilztjCIPfkkI/CJlM/K4WPllSwJoxgwBs1SZm1YMGgojVZP/tjpZXbUPT92en7251W1tTM9vf397pb09Lef79PfNndHRER6X1+7OyAiIq2hgC8ikhAK+CIiCaGALyKSEAr4IiIJsazdHajknHPO8QsvvLDd3RAR6Rr79u37hbsPhn3W0QH/wgsvZHJyst3dEBHpGmb2s6jPlNIREUkIBXwRkYRQwBcRSQgFfBGRhFDAFxFJiI6u0hHpZhNTObbuPsL0bJ4VAxnG1g8xOpxtd7ckwRTwRZpgYirHHdsPki/MAZCbzXPH9oMACvrSNgr4Ik2wdfeRhWAfyBfmuHvHYY36pW0U8EWaYHo2H7p9Nl9gNl8ANOqX1tOkrUgTrBjI1LRfvjDH1t1HmtwbkXkK+CJNMLZ+iEw6VdO+UVcDtZiYyrFuyx5Wje9k3ZY9TEzllnws6X1K6Yg0QZCiKc3XHzt+gteOFU7Zt9argXKaGJZ6KeCLNMnocHZR4C0P0ACZdIqx9UN1HTco98yFXBkEKSIFfAnTcErHzIbMbH/J1xtmdnvZPh80s9dL9rmz0XZFus3ocJbNG9ewvD+9sO30ZfX9FwxOGmHBPtBIikh6W8MjfHc/AqwFMLMUkAMeCdn179z9w422J9Itwm68Ani7cHJhn9l8oa40TFi5Z7mlpoik98Wd0vkd4Dl3j1yPWaTb1XIHbVR+/V3pvtD6/CANU+3Y1UbvS0kRSXLEHfBvBr4R8dlvm9kBYBr49+5+OGwnM9sEbAJYuXJlzN0TaczEVI6xhw5QmHNgPpCPPXQAWDxCj7rxKmp0npvN88cTB9n21EsVj71iIBOZzsnqRi6pwtw9ngOZncZ8MF/t7j8v++zXgJPu/qaZXQf8N3e/qNoxR0ZGXE+8kk4y/CePhVba9Bn8i99ayRPPzDA9myee/1Xz+tN9LD/jdKZn8/SfluKt44tPGpl0is0b1yjQCwBmts/dR8I+i7MO/1rgh+XBHsDd33D3N4uvdwFpMzsnxrZFWiIs2AOcdPj6ky+SqxLsBzJp0n1WV5vHCicXjlse7A248dKsgr3UJM6AfwsR6Rwz+3Uzs+Lry4rt/jLGtkU6Xiad4sMXnzcfpWPiwBPPzMR3QOlpsQR8M+sHPgRsL9n2STP7ZPHtx4BDxRz+nwM3e1y5JJEWGsikq+8UIjuQYfPGNTzxzMxCjj4uKsOUWsUS8N39mLu/291fL9l2n7vfV3x9r7uvdveL3f1yd/+/cbQr0mp3f2R13SkZA/aOXwlQsX5+qVSGKbXSnbYiNSgtlxzoT/N2YY58ST19JSsGMtz61b9n73OvNqVvV7xvsCnHld6jgC9SxR9PHOSBJ19cmIx97ViBWgf5mXSK/tP6mhbsAR7el2PkgrM1cStVabVMSaxaVpqcmMotCvaBkxFpeLP5fL0x//3GS7P85JW3Yu97qXxhjtu37ddqmVKVRviSSLWuNLl195G6aurd5/P1QQro60++GGe3K6pltUw9ZzfZFPAlkaLuhC1faXIpFTAX/dFOakzvx67SaplaTlmU0pFEigrk5duXUgHTrmAfiPrdKp3kJBkU8CWRogK5w6JceD1PruoUfWahufxaT3LSuxTwJZEqBfIg1TExlVtYwz7bRbXuc+4L/S8VdZJTHX9yxLZ4WjNo8TRppkpPjoL5u2rPOH0Zudk8fRZdmdOpUmacdF+0Fn/YE7e08FpvadXiaSJdZXQ4y97xKyOXtpnNFxZOBt0W7GF+pO8snpwNrlaCslEF+2RRlY4kXqU15ntFvjDHPd8+TP9py1SSmWAa4Uvija0fIp2KcQnLDvXascLCMsul8xSSHAr4IkCsTyzpEirJTB4FfEm8rbuPUOjGJH0MVJKZLAr4kni9HvSyA5nIdfxVkpksmrSVRCpdU6bPjLkOLk9uVKWSzOAzSQaN8CVxgjVlggnMXg72wMLaOirJFI3wJXHC1pTpZUHJ6eiwHnaedBrhS+L0es6+XMp6v+RUaqOAL4mTtInKXk9ZSe1iC/hm9oKZHTSz/WZ2ygI4Nu/PzexZM/uRmV0SV9si9QhbOC3dZzU/trDbdNPCb9JccY/wr3D3tREL91wLXFT82gR8Jea2RWoSNoF502Xn82vvCi9d7GaqxJFSrZy03QD8b59fnvNJMxsws/Pc/eUW9kEEWDyBWf4kqF7RZ3DjpZqolXfEOcJ34DEz22dmm0I+zwIvlbw/Wty2iJltMrNJM5ucmZmJsXsi4Xq1auekw8P7clovRxbEGfDXufslzKduPmVmHyj7PCxDespskrvf7+4j7j4yODgYY/dEwvVy1Y7Wy5FSsQV8d58ufn8FeAS4rGyXo8D5Je/fA0zH1b7IUlWr2un2udxePqFJfWIJ+GZ2hpmdGbwGrgYOle22A/hXxWqdy4HXlb+XTlDtubXdXtSYtDJUiRbXpO25wCM2f4PHMuCv3P1RM/skgLvfB+wCrgOeBY4Bvx9T2yINCSY1e3FtHVXpSCk901akzKrxnV0/qgdY3p/mrhtWq0onYfRMW5E69EoK5O3CyXZ3QTqMAr5ImWo5/W6hCh0pp4AvUqb0Ttxul5vNqw5fFmh5ZJEQpXfiXji+s829acwd2w8CKJcvGuGLVNPtI32ldiSgEb5IiNJHIL4r3f3jotxsnlXjO1kxkGFs/ZBG+wmlgC9SpnwxtXyPVLs484FfKZ7k6v6hi0jMenUxtYBSPMmlEb4kWmnqJkh35Lpo7ZkzTkvx1vH6T05aXyeZNMKXxApSN7nZ/EK6Y+xbB9rdrZplBzL86UfXkE7Vv7xbr9xcJvVRwJfECkvdFE5WX1Qhk06x7r1nt3UVzWCNnNHhLFs/dnFdlURaXye5FPAlsepNawSPQ7zx0iwv/HL+qiBl7Qn7mzeuWZh0HR3Osnf8Sr5009rI/VNmC/0v/VlJFuXwJbFWDGRqztenzHhu83WnVPC0Y1XNlFlowK40EXvSnZ9uub6Z3ZIuoBG+JFY9a+YEgX0pFTwDmfTCA9PjuCKIOslUumJRzl5AI3xJsPJ18FcMZDh2/ASvHSucsm+QI68UVPvTfRwrq9nPpFPc/ZF3liheFcMyDVEnjagrFgPl7AXQCF8SLsh//3TL9ewdv5K7blh9yqi/dJIzaqS8vD/N0//5Wr5009qF0XxYvjyOkXbUCD/sisWAWy9fqZy9ABrhiywSNuovXYpgbP0QYw8doDC3OOi++fYJJqZyixZdCzO2fmjRHECpVI1P2oqqyKnWdxE98UqkTmvveYzZfHjaZ+/4lVV/Puxmr9Hh7CkTwmEy6ZSqbKSiSk+80ghfpE6vhwR7qL3MM+oqIGyEfsX7BnnimRmN2CUWCvgidYqaHHVg3ZY9DQXlaikhkUY0PGlrZueb2RNm9mMzO2xm/y5knw+a2etmtr/4dWej7Yq0S6VyzmA1Sj1lSjpRHCP8E8Bn3f2HZnYmsM/MHnf3p8v2+zt3/3AM7Ym0VWnqJWykH6xGqZG6dJqGR/ju/rK7/7D4+lfAjwH9S5eeFpRzRt1GpdUopRPFWodvZhcCw8APQj7+bTM7YGbfMbPVFY6xycwmzWxyZmYmzu6JxC6qrl53tkonii3gm9k/Ah4Gbnf3N8o+/iFwgbtfDPx3YCLqOO5+v7uPuPvI4OBgXN0TaYqwfL5Wo5ROFUvAN7M088H+AXffXv65u7/h7m8WX+8C0mZ2Thxti7TT6HCWzRvXVLy7VqRTNDxpa2YG/AXwY3f/QsQ+vw783N3dzC5j/kTzy0bbFukEKqWUbhFHlc464PeAg2a2v7jtj4CVAO5+H/Ax4N+a2QkgD9zsnXyLr4hID2o44Lv796Hyw3/c/V7g3kbbEhGRpdNqmSIiCaGALyKSEAr4IiIJoYAvIpIQCvgiIgmhgC8ikhAK+CIiCaGALyKSEAr4IiIJoYAvIpIQCvgiIgmhgC8ikhAK+CIiCaGALyKSEAr4IiIJoYAvIpIQCvgiIgmhgC8ikhAK+CIiCRFLwDeza8zsiJk9a2bjIZ+fbmbbip//wMwujKNdERGpXcMB38xSwJeBa4H3A7eY2fvLdvsD4DV3/w3gi8B/abRdERGpz7IYjnEZ8Ky7Pw9gZt8ENgBPl+yzAbi7+Poh4F4zM3f3GNoXEelIE1M5tu4+wvRsnhUDGcbWDzE6nG1bf+JI6WSBl0reHy1uC93H3U8ArwPvjqFtEZGONDGV447tB8nN5nEgN5vnju0HmZjKta1PcYzwLWRb+ci9ln3mdzTbBGwCWLlyZWM9ExFpk627j5AvzC3ali/MsXX3kYXPWz3yj2OEfxQ4v+T9e4DpqH3MbBlwFvBq2MHc/X53H3H3kcHBwRi6JyLSOhNTOdZt2UNuNh/6eW42z9hDB9oy8o8j4D8FXGRmq8zsNOBmYEfZPjuATxRffwzYo/y9iPSa0jROJYW5xeGvdOTfTA0H/GJO/jZgN/Bj4EF3P2xmf2JmHynu9hfAu83sWeAzwCmlmyIi3S4sjVOr3Gy+6aP8OHL4uPsuYFfZtjtLXr8NfDyOtkREOtV0lZF9NXdsPwjQtHy+7rQVEakiyMuvGt/Jui17IkfiKwYyDbXT7NSOAr6ISAX1lFeOrR8ik0411F6jVwmVKOCLiFRQrbyy1Ohwls0b15BtYKTf6FVCJbHk8EVEelXUiDs3m2f1nY/y1vH5k4EB/+y9Z/PCL/NVq3QqGVs/tOSfrUYjfBGRCiqNuINgD/N3ku597tWGgv3y/nRTb8BSwBcRqSCOvHwtMukUd92wuqltKOCLiFQQR16+muX9aU5f1sent+2vWAXUKAV8EZEqRoez7B2/MnRRsDjMHiswmy80fakFBXwRkSqCOvxmrQdTftxm1eOrSkdEpIKgDn+pSyYsVTPq8TXCFxGpoJb1cZqR6mlGPb4CvohIBbWMtONO9RjNqcdXwBcRqaCZd76GMeDWy1c2pR5fAV9EpIIr3tfaBzHdevlKPje6pinHVsAXEangiWdmeqY9BXwRkQqauXplmNxsvuoyzEulgC8iUkGrc/hA027AUsAXEYkwMZXj2PETbWs/7huwdOOViEiIdt1wVS7OlJJG+CIiIRp5IHmc4kwpNTTCN7OtwA3AceA54PfdfTZkvxeAXwFzwAl3H2mkXRGRZmv1ZG2YTDoV6w1YjY7wHwd+093/KfD/gDsq7HuFu69VsBeRbtCOydpSZrB545pYb8BqKOC7+2PuHsxoPAm8p/EuiYi0X6sefBLJif1u2zhz+P8G+E7EZw48Zmb7zGxTpYOY2SYzmzSzyZmZ1t7wICIC8xO2QQ4/ZfNLozVrLfwoZ2XSsR+zasA3s++a2aGQrw0l+/xH4ATwQMRh1rn7JcC1wKfM7ANR7bn7/e4+4u4jg4OtvaVZRCSozgmeTTvnjhH/AmnVvHX8ROw3XlWdtHX3qyp9bmafAD4M/I67h/5N3H26+P0VM3sEuAz4Xv3dFRFprrDqnFYHe4DCnLN195HOyeGb2TXAfwA+4u7HIvY5w8zODF4DVwOHGmlXRKRZOqE6JxB3XxrN4d8LnAk8bmb7zew+ADNbYWa7ivucC3zfzA4A/wDsdPdHG2xXRKQpoqpzWp3Dh/grhRqqw3f334jYPg1cV3z9PHBxI+2IiLTK2PqhU+6wzaRTXLLyLPY+92rL+hF3DT7oTlsRkUVGh7Ns3riG7EAGA7IDGTZvXMPTL/+qZX0I2oy7LFNr6YiIlBkdzi4KthNTOV47VmhJ2wbsHb+yKcfWCF9EpIo4V6ysppl3+Crgi4hU0arKnWY9vDyggC8iUkWr1tVp1sPLAwr4IiJVtGJdnXQfjFxwdlPbsIibYzvCyMiIT05OtrsbIiIL6+vkWpDeyQ5kGFs/tKTRvpnti1qVWCN8EZEajA5nm1Y9U64Zz7MFBXwRkbr0teiW27ifZwsK+CIidTl9WevCZqetpSMikihvF062rK2OWktHRCQJggnb6dk8fWbMhRS7pMzoM6fW84EBA/3pyDt4m7GWjgK+iEgFwQNRgsXUwoJ9Jp1i88Y13L3jMLP52pZgcGDqzqsXtROcVFY0UKVTiQK+iEgFYQ9EgfkR/Un3RcH509v213zcbFm6pnz9nmZQwBcRqSBq4vSkOz/dcv2ibSsGMqF1+uWPSGxGuqYWmrQVEakgauK0z4xV4ztZt2XPQr182B25mXSKWy9fecpyy80ezYfRCF9EpIKwB6LAO7n84CYpYCGINzsXv1QK+CIiFZQH8bAqneAmqSAP3ykBvpwCvohIFaVBfNX4ztB9Ounh51GUwxcRqUNUTr9VSyg3oqGAb2Z3m1nOzPYXv66L2O8aMztiZs+a2XgjbYqItFPUxGw7qm7qFUdK54vu/l+jPjSzFPBl4EPAUeApM9vh7k/H0LaISEt1+sRsJa3I4V8GPOvuzwOY2TeBDYACvoh0pU6emK0kjhz+bWb2IzP7mpktD/k8C7xU8v5ocVsoM9tkZpNmNjkzMxND90REBGoI+Gb2XTM7FPK1AfgK8F5gLfAy8PmwQ4Rsi3zMlrvf7+4j7j4yODhY468hIiLVVE3puPtVtRzIzL4K/E3IR0eB80vevweYrql3IiISm0ardM4reftR4FDIbk8BF5nZKjM7DbgZ2NFIuyIiUr9GJ23/zMzWMp+ieQH4QwAzWwH8T3e/zt1PmNltwG4gBXzN3Q832K6IiNSpoYDv7r8XsX0auK7k/S5gVyNtiYhIY3SnrYhIQijgi4gkhAK+iEhCKOCLiCSEAr6ISEIo4IuIJIQCvohIQijgi4gkhAK+iEhCKOCLiCSEAr6ISEIo4IuIJIQCvohIQrTimbYSk4mpXFc+OFlEOoMCfhPFGaAnpnLcsf0g+cIcALnZPHdsPwigoC8iNVHAX6JqwTzuAL1195GFYwXyhTm27j6igC8iNVHAX4JagnlUgP7sgwcW3tcz+s/N5uvaLiJSTgF/CWoZbU9HBOI5d8YeOgAOhZMO1Db6T5kx5x66XUSkFqrSWYKoYF66fcVAJvLnC3O+EOwDwQkjSliwr7R9qSamcqzbsodV4ztZt2UPE1O5WI8vIu2jgL8EUcG8dPvY+iFSffWNvsNOJBNTOdbe81jkz/QZsQXlIFWVm83jvHPloaAv0hsaSumY2TZgqPh2AJh197Uh+70A/AqYA064+0gj7daqWWWMV7xvkK8/+WLo9sC3Jl9k7mR9o+8+Myamcgt9nJjKMfatA6dcDZQ66cyniIoa+X01MSzS2xoK+O5+U/DazD4PvF5h9yvc/ReNtFePOKpkwk4YAN/4wUuh+z/xzAwAt37179n73Kt193nOfVEft+4+UjHYBwpzzu3b9mNAsPdSft9aUlUi0r1imbQ1MwN+F7gyjuPFodpodSlllcFka1TePDeb55/8p++QL5xccr9Lc/n1VuCU96q0Kqj0qiHq914xkAlts9J8hIh0j7iqdP458HN3/0nE5w48ZmYO/A93vz/qQGa2CdgEsHLlyiV3qNJodalllYW56qPtRoJ9IDebZ+xbB6rvWIPSqwaYT/8Ev8fCSYz533ts/dCizwHSKVu4shGR7lZ10tbMvmtmh0K+NpTsdgvwjQqHWefulwDXAp8ysw9E7eju97v7iLuPDA4ORu1WVaWJ1Uqj/0C769trSeXUKl+Y4+4dh/nMg/tPOWkV5px7vn34nQ3lzcZbBCQibVQ14Lv7Ve7+myFffw1gZsuAjcC2CseYLn5/BXgEuCye7kcbWz9EJp1atC2TTjG2fqhqrnpiKkevVbfP5gtEnUNeO1YACJ0zKJz0iuWiItI94ijLvAp4xt2Phn1oZmeY2ZnBa+Bq4FAM7VY0OpzlxkuzCzcmpcy48dIso8PZyNH/WZk067bs4fZt+xM3sJ2YytU0aas6fZHuFUfAv5mydI6ZrTCzXcW35wLfN7MDwD8AO9390RjarWhiKsfD+3ILE6xz7jy8Lzdf6hgy+gd44+1C21M57fLpCie54ASpOn2R7tZwwHf3f+3u95Vtm3b364qvn3f3i4tfq939TxttsxbVqnRuvDR7StqmlrR5us6bqZol7l5E/epBGgwq/01FpPP17J221dITTzwzU3faJpPuiz/SLkG6z7j18pU0+9yTHchw46Xz9wOsGt8ZefWjOn2R7tCzAb/a8gf1Bqnl/WnOPuP0mkozm85g5IKz+cLvriWdak7UN+Ynvh/el1tI4URRnb5Id+jZgF+pSgfqC1KZdIq7bljdMSPZ4M7a27edWmYZZXl/uq42ospXwxw7fkJ5fJEu0LMBf3Q4y+aNa8gOZDDm0xObN65ZuLEq7ISQThkDmfnAGFT3lP5cJ49kK43zl/eneb1YelnrsSqVr5Z77VhBk7ciXcA85uV14zQyMuKTk5NNO369i6uV36EL86P/twtzPVfG+cKW61m3ZU9dVUvZgQx7xztmdQ2RRDKzfVELVCb6ASijw9m6VoEsXXah9CQx+bNXQ1fPTPVZ3StmdoJs8UpmbP3QKSe4Sjol5SUi4Xou4DdrSeRA+UliYiq3sEpmIGXGLb91PiMXnM3t2/bH1vZSZNKpmgM2LF47p/wEd1Ymjdk7d+aW6+SUl4j0WMCP+8Hh9bYXOH2Z8cCTL/LEMzMMZNLM5mvPn6f7rOF1dIJlkrPFE9493z4cGaQz6b6FBd+W96e564bVi/5WYVdBUaktLbIm0tl6KuC3+gEeUVUsx4oBNDebJ52ymoN4EKCDEfVAfxp3eD1foC/imbZhnPmrjNKrm/JVMAH+5eUr+dzompqOWSoqtaWHpIh0tp4K+K1+gEctxy3MOcv70/SftqziBGhQGRM1rxB1NRGl/GEqEG+Arnf+Q0Tar6cCfqsf4BHVXrnZYwWm7rwagA994f/wk1feWvS5AbdevrJiAC0P2gP9ad58+0TFK4fSqxsFaBHpqTr8ajdbNaO9Wu5zLT3hPP6ZD/Klm9Yuuj/gizetrSm1MjqcZe/4lfx0y/VM3Xk1Wz9+8cJ9A1FUOSMigZ4a4bc6tzw6nGXyZ6/ywJMv1rT4WOnPxdGn4DgTUzk+++CB0By/KmdEJNBTAR9an1v+3OgaRi44O3SitVWTmcHxVTkjIpX0XMBvh07Ij6tyRkSqUcDvIZ1w4hGRztVTk7YiIhJNAV9EJCEU8EVEEkIBX0QkIRTwRUQSoqMfgGJmM8DP2t2PDnQO8It2d6IL6O9UO/2tatMNf6cL3H0w7IOODvgSzswmo55oI+/Q36l2+lvVptv/TkrpiIgkhAK+iEhCKOB3p/vb3YEuob9T7fS3qk1X/52UwxcRSQiN8EVEEkIBX0QkIRTwu5SZ3W1mOTPbX/y6rt196iRmdo2ZHTGzZ81svN396VRm9oKZHSz+G5psd386iZl9zcxeMbNDJdvONrPHzewnxe/L29nHeingd7cvuvva4teudnemU5hZCvgycC3wfuAWM3t/e3vV0a4o/hvq2vryJvlfwDVl28aBv3X3i4C/Lb7vGgr40osuA5519+fd/TjwTWBDm/skXcbdvwe8WrZ5A/CXxdd/CYy2tFMNUsDvbreZ2Y+Kl55ddWnZZFngpZL3R4vb5FQOPGZm+8xsU7s70wXOdfeXAYrf/3Gb+1MXBfwOZmbfNbNDIV8bgK8A7wXWAi8Dn29rZzuLhWxT/XG4de5+CfPpr0+Z2Qfa3SFpHj3isIO5+1W17GdmXwX+psnd6SZHgfNL3r8HmG5TXzqau08Xv79iZo8wnw77Xnt71dF+bmbnufvLZnYe8Eq7O1QPjfC7VPEfW+CjwKGofRPoKeAiM1tlZqcBNwM72tynjmNmZ5jZmcFr4Gr076iaHcAniq8/Afx1G/tSN43wu9efmdla5lMVLwB/2N7udA53P2FmtwG7gRTwNXc/3OZudaJzgUfMDOZjwV+5+6Pt7VLnMLNvAB8EzjGzo8BdwBbgQTP7A+BF4OPt62H9tLSCiEhCKKUjIpIQCvgiIgmhgC8ikhAK+CIiCaGALyKSEAr4IiIJoYAvIpIQ/x/VT56xdFzlfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_2d_sample(sample):\n",
    "    sample_np = sample.detach().numpy()\n",
    "    x = sample_np[:, 0]\n",
    "    y = sample_np[:, 1]\n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "plot_2d_sample(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model using the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of point that is assigned to each gaussian: \n",
      "tensor([[[353.2728]],\n",
      "\n",
      "        [[408.3874]],\n",
      "\n",
      "        [[238.3398]]], grad_fn=<ViewBackward>)\n",
      "0  iter:\n",
      "mu:  tensor([[ 2.7800,  5.3981],\n",
      "        [ 6.8489,  1.2731],\n",
      "        [-1.7646, -1.6247]], grad_fn=<SqueezeBackward1>)\n",
      "pi:  tensor([0.3533, 0.4084, 0.2383], grad_fn=<SqueezeBackward0>)\n",
      "logvar:  tensor([[0.2715, 0.3035],\n",
      "        [2.8221, 2.6607],\n",
      "        [2.8067, 3.7475]], grad_fn=<SqueezeBackward1>)\n",
      "----------------------------------\n",
      "num of point that is assigned to each gaussian: \n",
      "tensor([[[613.5221]],\n",
      "\n",
      "        [[244.0715]],\n",
      "\n",
      "        [[142.4064]]], grad_fn=<ViewBackward>)\n",
      "1  iter:\n",
      "mu:  tensor([[ 2.6010,  5.2944],\n",
      "        [ 9.7088, -1.3567],\n",
      "        [-4.2618, -6.1612]], grad_fn=<SqueezeBackward1>)\n",
      "pi:  tensor([0.6135, 0.2441, 0.1424], grad_fn=<SqueezeBackward0>)\n",
      "logvar:  tensor([[-0.6749, -0.3807],\n",
      "        [ 2.0885,  1.9243],\n",
      "        [ 2.5788,  3.0038]], grad_fn=<SqueezeBackward1>)\n",
      "----------------------------------\n",
      "num of point that is assigned to each gaussian: \n",
      "tensor([[[660.7787]],\n",
      "\n",
      "        [[216.1605]],\n",
      "\n",
      "        [[123.0609]]], grad_fn=<ViewBackward>)\n",
      "2  iter:\n",
      "mu:  tensor([[ 2.5978,  5.2766],\n",
      "        [10.7333, -2.2517],\n",
      "        [-5.5109, -7.8029]], grad_fn=<SqueezeBackward1>)\n",
      "pi:  tensor([0.6608, 0.2162, 0.1231], grad_fn=<SqueezeBackward0>)\n",
      "logvar:  tensor([[-0.6484, -0.3292],\n",
      "        [-0.3807,  0.0910],\n",
      "        [ 0.9274,  0.9448]], grad_fn=<SqueezeBackward1>)\n",
      "----------------------------------\n",
      "num of point that is assigned to each gaussian: \n",
      "tensor([[[665.]],\n",
      "\n",
      "        [[214.]],\n",
      "\n",
      "        [[121.]]], grad_fn=<ViewBackward>)\n",
      "3  iter:\n",
      "mu:  tensor([[ 2.5989,  5.2680],\n",
      "        [10.8054, -2.3139],\n",
      "        [-5.6375, -8.0025]], grad_fn=<SqueezeBackward1>)\n",
      "pi:  tensor([0.6650, 0.2140, 0.1210], grad_fn=<SqueezeBackward0>)\n",
      "logvar:  tensor([[-0.6326, -0.3086],\n",
      "        [-1.8091, -0.3357],\n",
      "        [ 0.4610, -1.5967]], grad_fn=<SqueezeBackward1>)\n",
      "----------------------------------\n",
      "num of point that is assigned to each gaussian: \n",
      "tensor([[[665.]],\n",
      "\n",
      "        [[214.]],\n",
      "\n",
      "        [[121.]]], grad_fn=<ViewBackward>)\n",
      "4  iter:\n",
      "mu:  tensor([[ 2.5989,  5.2680],\n",
      "        [10.8054, -2.3139],\n",
      "        [-5.6375, -8.0025]], grad_fn=<SqueezeBackward1>)\n",
      "pi:  tensor([0.6650, 0.2140, 0.1210], grad_fn=<SqueezeBackward0>)\n",
      "logvar:  tensor([[-0.6326, -0.3086],\n",
      "        [-1.8091, -0.3357],\n",
      "        [ 0.4610, -1.5967]], grad_fn=<SqueezeBackward1>)\n",
      "----------------------------------\n",
      "fit done!\n"
     ]
    }
   ],
   "source": [
    "features = 3\n",
    "dim = 2\n",
    "mu, var, pi = initialize(data, features, var=1)\n",
    "model = GMM(features, dim, mu, var)\n",
    "model.fitGMM(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0010, 0.0010],\n",
      "        [0.0010, 0.0010],\n",
      "        [0.0010, 0.0010],\n",
      "        [0.0010, 0.0010],\n",
      "        [0.0010, 0.0010],\n",
      "        [0.0010, 0.0010],\n",
      "        [0.0010, 0.0010],\n",
      "        [0.0010, 0.0010],\n",
      "        [0.0010, 0.0010]])\n"
     ]
    }
   ],
   "source": [
    "# test gradient: \n",
    "# assume obj = mean(y); that is obj = [1/(n_dim * n_samples)] * y \n",
    "# derivate = 1/(1000 * 2) -->(0.0005)\n",
    "obj =  data.mean()\n",
    "obj.backward()\n",
    "print(data.grad[1:10,])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
